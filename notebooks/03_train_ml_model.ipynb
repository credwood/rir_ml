{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/credwood/rir_ml.git"
      ],
      "metadata": {
        "id": "mRHp6NGWnd1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r rir_ml/requirements.txt"
      ],
      "metadata": {
        "id": "XoQ6aJSUokW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/rir_ml')\n"
      ],
      "metadata": {
        "id": "Eu6BsWkvowfo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lVj8343jncYE"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "from core.dataset_utils import RIRHDF5Dataset, denormalize, convert_to_db\n",
        "from core.models import CNN1D\n",
        "from core.training_utils import WeightedMSELoss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access your data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Now you can use your HDF5 files like this:\n",
        "rir_path = '/content/drive/MyDrive/rir_data/rir_dataset.h5'\n",
        "metrics_path = '/content/drive/MyDrive/rir_data/rir_metrics.h5'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9__Ya0sjnoTR",
        "outputId": "dee1df39-ceaf-4a91-cfb9-e47ecd4510f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_HAooShAncYF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Full dataset\n",
        "full_dataset = RIRHDF5Dataset(rir_path, metrics_path)\n",
        "\n",
        "indices = list(range(len(full_dataset)))\n",
        "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Get train targets\n",
        "train_targets = np.stack([full_dataset[i][1].numpy() for i in train_idx])\n",
        "target_mean = train_targets.mean(axis=0)\n",
        "target_std = train_targets.std(axis=0)\n",
        "\n",
        "train_set = RIRHDF5Dataset(\n",
        "    rir_path, metrics_path,\n",
        "    normalize_targets=True,\n",
        "    target_mean=target_mean,\n",
        "    target_std=target_std,\n",
        "    subset_indices=train_idx\n",
        ")\n",
        "\n",
        "val_set = RIRHDF5Dataset(\n",
        "    rir_path, metrics_path,\n",
        "    normalize_targets=True,\n",
        "    target_mean=target_mean,\n",
        "    target_std=target_std,\n",
        "    subset_indices=val_idx\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1Wx0kri1ncYF"
      },
      "outputs": [],
      "source": [
        "# Set up log file path\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_dir = \"training_logs\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, f\"train_{timestamp}.log\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "w_b5h30ZncYF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output, display\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def init_weights_kaiming(m):\n",
        "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
        "        init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "def plot_metrics(history):\n",
        "    \"\"\"\n",
        "    Plots training and validation loss, and real-world MAE curves for all metrics.\n",
        "    Ensures integer epoch ticks.\n",
        "    \"\"\"\n",
        "    epochs = list(range(1, len(history['train_loss']) + 1))\n",
        "\n",
        "    # --- Plot Training and Validation Loss ---\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    ax = sns.lineplot(x=epochs, y=history['train_loss'], label='Train Loss')\n",
        "    sns.lineplot(x=epochs, y=history['val_loss'], label='Val Loss')\n",
        "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Plot Real-World MAE (Seconds) ---\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    ax = sns.lineplot(x=epochs, y=history['mae_rt60'], label='RT60 (s)')\n",
        "    sns.lineplot(x=epochs, y=history['mae_edt'], label='EDT (s)')\n",
        "    sns.lineplot(x=epochs, y=history['mae_d50'], label='D50')\n",
        "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    plt.title(\"Real-World MAE — Time-Based Metrics\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"MAE (seconds)\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Plot Real-World MAE (Decibels) ---\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    ax = sns.lineplot(x=epochs, y=history['mae_c50'], label='C50 (dB)', color='orange')\n",
        "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    plt.title(\"Real-World MAE — C50\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"MAE (dB)\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset, num_epochs=20, batch_size=64, lr=1e-4, device='cuda'):\n",
        "    \"\"\"\n",
        "    Trains the model on normalized metrics, using AdamW and validation-based LR scheduler.\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3\n",
        "    )\n",
        "    metric_weights = torch.tensor([0.5, 3.0, 2.0, 0.5])\n",
        "    criterion = WeightedMSELoss(metric_weights).to(device)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    best_val_loss = float(\"inf\")\n",
        "    history = defaultdict(list)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for rirs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
        "            rirs, targets = rirs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(rirs.unsqueeze(1))  # [B, 1, N]\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        mae_sum = torch.zeros(4, device=device)\n",
        "        num_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for rirs, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
        "                rirs, targets = rirs.to(device), targets.to(device)\n",
        "                outputs = model(rirs.unsqueeze(1))\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Denormalize for metric reporting\n",
        "                # Denormalize to linear space\n",
        "                outputs_real = denormalize(outputs, val_dataset.target_mean, val_dataset.target_std)\n",
        "                targets_real = denormalize(targets, val_dataset.target_mean, val_dataset.target_std)\n",
        "\n",
        "                # Convert C50/D50 to dB\n",
        "                outputs_db = convert_to_db(outputs_real, indices=[2])\n",
        "                targets_db = convert_to_db(targets_real, indices=[2])\n",
        "\n",
        "                # Compute MAE in real-world scale\n",
        "                mae_batch = torch.mean(torch.abs(outputs_db - targets_db), dim=0)\n",
        "\n",
        "\n",
        "                mae_sum += mae_batch\n",
        "                num_batches += 1\n",
        "\n",
        "        avg_val_loss = val_loss / num_batches\n",
        "        avg_mae = (mae_sum / num_batches).cpu().numpy()\n",
        "\n",
        "        scheduler.step(avg_val_loss)\n",
        "        best_val_loss = min(best_val_loss, avg_val_loss)\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), \"best_cnn_model.pt\")\n",
        "            logger.info(f\"New best model saved at epoch {epoch+1} with val loss {avg_val_loss:.4f}\")\n",
        "\n",
        "        logger.info(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "        logger.info(f\"MAE — RT60: {avg_mae[0]:.4f}s | EDT: {avg_mae[1]:.4f}s | C50: {avg_mae[2]:.2f}dB | D50: {avg_mae[3]:.3f}\")\n",
        "        history[\"epoch\"].append(epoch + 1)\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        history[\"val_loss\"].append(avg_val_loss)\n",
        "        history[\"mae_rt60\"].append(avg_mae[0])\n",
        "        history[\"mae_edt\"].append(avg_mae[1])\n",
        "        history[\"mae_c50\"].append(avg_mae[2])\n",
        "        history[\"mae_d50\"].append(avg_mae[3])\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"Real-World MAE: RT60={avg_mae[0]:.4f}s, EDT={avg_mae[1]:.4f}s, C50={avg_mae[2]:.2f}dB, D50={avg_mae[3]:.3f}\")\n",
        "\n",
        "        plot_metrics(history)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yphsFSywncYF"
      },
      "outputs": [],
      "source": [
        "model = CNN1D()\n",
        "model.apply(init_weights_kaiming)\n",
        "model = train_model(model, train_set, val_set, num_epochs=20, batch_size=1024, lr=1e-5, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieyO6e8uncYF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}